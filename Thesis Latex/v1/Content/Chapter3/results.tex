\section{Results of the Runtime Verified AV System}

This research provides a solution for two aspects of \acf{AV} systems: predicting accurately with perturbations to the system's inputs and safely dealing with misclassifications by the system.
The issue of input perturbations was addressed using a \acf{MNN} of different convolutional \acfp{SNN}, each \ac{SNN} working in tandem to predict more accurately.
Misclassification by the system's controller was addressed by implementing sensor fusion between cameras and \ac{LiDAR}.
This was done using a run-time enforcer that enforced a safety automaton.

\todo{fix results with new graphs}

To test the \ac{MNN}'s ability to deal with perturbations, the input images (taken from the \ac{VOC} 2012 and \ac{GTSRB} datasets) were perturbed by randomly replacing approximately 7\% of the image pixels with randomly coloured pixels.
Table~\ref{tbl:sign-res} shows that without perturbations, the accuracy of the \ac{MNN} was increased from 87.07\% to 93.7\% when using an enforced policy.
Table~\ref{tbl:sign-respert} shows that the accuracy of the \ac{MNN} hugely decreases when perturbations are present, from 87.07\% to 41.6\%.
However, with sensor fusion and a relative safety policy, the accuracy is increased to 90.48\%.

\begin{table}[h]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|p{0.17\linewidth}||p{0.17\linewidth}|p{0.17\linewidth}|p{0.17\linewidth}|p{0.17\linewidth}|}
			\hline
			Epochs trained & No. of misclassifications (\%) & Caught misclassifications (\%) & False negatives (\%) & Uncaught misclassifications (\%) \\ \hline
			\multicolumn{5}{|l|}{Original Inputs} \\ \hline
			0 & 95.16 & 95.16 & 4.84 & 4.84 \\ 
			10 & 95.16 & 95.16 & 4.84 & 4.84 \\
			100 & 82.67 & 61.09 & 1.83 & 23.41 \\
			1000 & 29.36 & 21.39 & 2.92 & 10.89 \\
			10000 & 12.38 & 8.55 & 2.11 & 5.93 \\ 
			100000 & 11.98 & 7.79 & 2.13 & 6.33 \\
			6000 (best) & 10.59 & 7.32 & 2.04 & 5.31 \\ \hline
			\multicolumn{5}{|l|}{Perturbed Inputs} \\ \hline
			0 & 95.16 & 95.16 & 4.84 & 4.84 \\
			10 & 95.16 & 95.16 & 4.84 & 4.84 \\ 
			100 & 93.63 & 71.89 & 0.56 & 22.29 \\
			1000 & 76.69 & 63.71 & 1.25 & 14.23 \\
			10000 & 57.89 & 45.89 & 2.27 & 14.28 \\ 
			100000 & 58.03 & 45.72 & 2.39 & 14.69 \\
			7000 (best) & 60.42 & 49.13 & 2.20 & 13.49 \\ \hline
		\end{tabular}%
	}
	\caption{Table showing the results of the \ac{AV} prediction \ac{SNN}}
	\label{tbl:sign-resultsfull}
\end{table}


\begin{figure}[t]
	\centering
	\scalebox{1}{\input{Content/fig/sign-graph.tex}}
	\caption{Line graph showing the performance of the system trained over an increasing amount of epochs using unperturbed inputs \label{fig:sign-graph}}
\end{figure}

\begin{figure}[t]
	\centering
	\scalebox{1}{\input{Content/fig/sign-graphpert.tex}}
	\caption{Line graph showing the number of misclassifications made by the system with perturbed inputs \label{fig:sign-graphpert}}
\end{figure}

\begin{figure}[t]
	\centering
	\scalebox{1}{\input{Content/fig/sign-graphboth.tex}}
	\caption{Line graph showing the number of misclassifications made by the system with all inputs \label{fig:sign-graphboth}}
\end{figure}

%\todo{Add table showing how \ac{MNN} affects the prediction accuracy}

\subsection{An \ac{AV} System Using \acf{MNN2C}}
\ac{MNN2C}, introduced in Section~\ref{sec:mnn2c}, creates time-predictable, modular \acfp{MNN} for C from existing Keras (with Tensorflow) trained \acp{ANN}. 
This compiler makes implementing \acfp{SNN} in C easy and safe.
For the purposes of testing and demonstration, the complex \ac{MNN} used in this chapter, shown in Figure~\ref{fig:mnn}, was trained in Python, using Keras and the exact same images used to train the original system.
This \ac{MNN} was then described in the \ac{MNN2C} format, and modular C code was generated to initialise, run and incorporate the \ac{MNN}.
To show the efficacy of \ac{MNN2C}, the generated \ac{MNN} was implemented in an identical system to the original, and run with the same tests. 
It has already been shown that \ac{MNN2C} generates outputs identical to the Keras trained \acp{ANN} with a one hundred-thousandth tolerance, so the output of each individual \ac{ANN} is not being tested, rather that the system as a whole runs as the original does.
Additionally, the time-predictable \ac{MNN} was run through the Patmos \ac{WCET} tool in order to get the \ac{WCET} and \ac{WCRT} of the \ac{MNN}.

\subsubsection{Results of an \ac{MNN2C} Generated \ac{AV} System}
\begin{table}[h]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|p{0.17\linewidth}||p{0.17\linewidth}|p{0.17\linewidth}|p{0.17\linewidth}|p{0.17\linewidth}|}
			\hline
			Epochs trained & No. of misclassifications (\%) & Caught misclassifications (\%) & False negatives (\%) & Uncaught misclassifications (\%) \\ \hline
			\multicolumn{5}{|c|}{Original Inputs} \\ \hline
			100 & 25.01 & 24.22 & 17.50 & 18.29 \\ \hline
			\multicolumn{5}{|c|}{Perturbed Inputs} \\ \hline
			100 & 41.95 & 39.61 & 21.44 & 23.78 \\ \hline 
		\end{tabular}%
	}
	\caption{Table showing the results of the \ac{AV} prediction \ac{SNN}}
	\label{tbl:sign-resultsfullmnn2c}
\end{table}





















