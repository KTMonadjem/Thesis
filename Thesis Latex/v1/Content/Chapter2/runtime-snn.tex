\section{Related Work}
\label{sec:related}


%More recently, progress has been made in the field of verification of Deep \acp{ANN}. 
Verification of \ac{ANN}, specifically, \acfp{DNN}, can be performed for properties (such as robustness and accuracy) using \ac{SMT}~\cite{Gehr2018AI2SA,reluplex,DeepANNverify}. 
Using \ac{SMT} and other techniques, certain safety properties of an \ac{ANN}, such as robustness, can be demonstrated. 
This is useful, because the robustness of a deep \ac{ANN} is a critical to its safety.
A robust \ac{ANN} is one that will provide consistently accurate outputs even when the input to the \ac{ANN} is noisy, incorrectly coloured or otherwise distorted. 
However, \ac{SMT} has issues with scale: as the \acp{ANN} to analyse become larger, analysis time grows exponentially~\cite{Gehr2018AI2SA}, making it very difficult to verify large, deep \acp{ANN}.
%Ergo, they are less efficient on larger deep \acp{ANN}.
In addition, they require the deep \acp{ANN} to meet a set of design constraints, requiring specific activation functions and limiting the \ac{ANN} architectures.
For instance in \cite{Gehr2018AI2SA}, where the robustness of \acfp{CNN} is verified by using an abstract technique, only \acp{CNN} and \acp{MLP} \acp{ANN} with the \ac{ReLU} activation functions can be statically analysed.
This limits flexibility, as each \ac{ANN} must be designed around these restrictions. % thus limiting properties of the \ac{ANN}, such as its activation function and size, could result in an \ac{ANN} that is inefficient, not robust, slow, etc.

 






